# ðŸ§  Toxicity Detection System - Technical Overview

## Overview

Your SafeSpace AI project uses **local ML models** for toxicity detection, specifically leveraging **Hugging Face Transformers** with the `unitary/toxic-bert` model. This is a **completely free, privacy-preserving solution** that runs locally on your server without sending data to external APIs (unless you configure it to use OpenAI).

---

## ðŸŽ¯ Architecture

### Multi-Tier Content Analysis System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Discord Message Received                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Quick Local Scan (Keyword-Based)                â”‚
â”‚  â€¢ Fast pre-filtering using risk keywords                    â”‚
â”‚  â€¢ Returns risk score (0.0 - 1.0)                           â”‚
â”‚  â€¢ Decides if full ML analysis is needed                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Content Analyzer Selection (config-based)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ HuggingFace  â”‚   OpenAI     â”‚  Mock/Fallback  â”‚         â”‚
â”‚  â”‚  (Default)   â”‚  (Optional)  â”‚   (Emergency)   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HuggingFace Analyzer (Primary)                  â”‚
â”‚  â€¢ Model: unitary/toxic-bert                                â”‚
â”‚  â€¢ Runs locally on CPU                                      â”‚
â”‚  â€¢ Multi-category classification                            â”‚
â”‚  â€¢ Keyword enhancement layer                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Analysis Result Processing                      â”‚
â”‚  â€¢ Threshold comparison (server-specific)                   â”‚
â”‚  â€¢ Violation categorization                                 â”‚
â”‚  â€¢ Action determination (warn/delete/timeout)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Adaptive Learning Feedback Loop                 â”‚
â”‚  â€¢ Tracks false positives                                   â”‚
â”‚  â€¢ Adjusts thresholds automatically                         â”‚
â”‚  â€¢ Learns server-specific culture                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ¤— HuggingFace Analyzer - The Core ML Model

### Model Details

**Model:** `unitary/toxic-bert`
- **Type:** BERT-based text classification model
- **Training:** Fine-tuned on toxic comment datasets
- **Categories Detected:**
  - `toxic` - General toxicity
  - `severe_toxic` - Extremely toxic content
  - `obscene` - Obscene language
  - `threat` - Threatening content
  - `insult` - Insulting language
  - `identity_hate` - Identity-based hate speech

### How It Works

#### 1. **Model Initialization** (Lazy Loading)
```python
# Located in: backend/app/ml/huggingface_analyzer.py

async def _initialize_models(self):
    """Initialize Hugging Face models asynchronously"""
    
    # Load the model in a separate thread to avoid blocking
    def load_model():
        return pipeline(
            "text-classification",
            model="unitary/toxic-bert",
            device=-1,  # Use CPU (set to 0 for GPU)
            top_k=None  # Return all category scores
        )
    
    self.toxicity_classifier = await self._run_in_thread(load_model)
```

**Key Points:**
- Model is loaded **on first use** (lazy initialization)
- Runs on **CPU by default** (no GPU required)
- Uses **async/await** to prevent blocking the Discord bot
- Downloads model files to local cache on first run (~400MB)

#### 2. **Text Analysis Process**

```python
async def analyze_text(self, text: str) -> Dict[str, Any]:
    # Step 1: Truncate long text (BERT has 512 token limit)
    if len(text) > 400:
        text = text[:400] + "..."
    
    # Step 2: Run BERT classifier
    toxicity_results = await self._run_in_thread(run_classifier)
    # Returns: [{'label': 'toxic', 'score': 0.989}, ...]
    
    # Step 3: Extract scores for each category
    toxic_score = scores_dict.get('toxic', 0.0)
    insult_score = scores_dict.get('insult', 0.0)
    obscene_score = scores_dict.get('obscene', 0.0)
    severe_toxic_score = scores_dict.get('severe_toxic', 0.0)
    identity_hate_score = scores_dict.get('identity_hate', 0.0)
    threat_score = scores_dict.get('threat', 0.0)
    
    # Step 4: Enhanced keyword detection
    nsfw_detected = self._check_nsfw_keywords(text)
    harassment_detected = self._check_harassment_keywords(text)
    threat_detected = self._check_threat_keywords(text)
    
    # Step 5: Create OpenAI-compatible response
    categories = {
        'harassment': harassment_detected or is_insulting,
        'hate': is_hateful or is_severe,
        'sexual': nsfw_detected or is_obscene,
        'violence': threat_detected or is_threatening,
        # ... more categories
    }
    
    # Step 6: Apply keyword adjustments
    adjusted_scores = self._apply_keyword_adjustments(scores, text)
    
    return analysis_result
```

#### 3. **Keyword Enhancement Layer**

The system combines ML predictions with keyword detection for better accuracy:

```python
def _apply_keyword_adjustments(self, scores: dict, text: str) -> dict:
    """Apply custom keyword confidence adjustments"""
    
    # Friendly context detection
    friendly_context = ['lol', 'haha', 'jk', 'just kidding', 'ðŸ˜‚']
    
    # Toxic words that should NEVER be reduced
    toxic_words = [
        'idiot', 'stupid', 'moron', 'fuck', 'shit', 
        'hate', 'kill', 'nazi', 'rape', 'faggot'
    ]
    
    # Reduce scores for friendly banter (only if no toxic words)
    if has_friendly_context and not has_toxic_words:
        adjustment_factor *= 0.8  # 20% reduction
    
    # Protect toxic content - never reduce below 85%
    if has_toxic_words:
        adjustment_factor = max(adjustment_factor, 0.85)
    
    return adjusted_scores
```

---

## ðŸ“Š Output Format

The analyzer returns an OpenAI-compatible response:

```json
{
  "flagged": true,
  "categories": {
    "harassment": true,
    "harassment/threatening": false,
    "hate": false,
    "sexual": false,
    "violence": false,
    "self-harm": false
  },
  "scores": {
    "harassment": 0.89,
    "harassment/threatening": 0.12,
    "hate": 0.34,
    "sexual": 0.05,
    "violence": 0.08,
    "self-harm": 0.02
  },
  "max_score": 0.89,
  "violation_type": "toxicity",
  "model_info": {
    "provider": "Hugging Face",
    "model": "unitary/toxic-bert",
    "toxic_score": 0.85,
    "insult_score": 0.89,
    "threat_score": 0.12,
    "version": "multi_category_with_adjustments"
  }
}
```

---

## ðŸŽ›ï¸ Configuration System

### Analyzer Selection

Located in `backend/app/utils/config.py`:

```python
CONTENT_ANALYZER: str = os.getenv("CONTENT_ANALYZER", "huggingface")
```

**Options:**
- `"huggingface"` or `"hf"` - Use local Hugging Face model (default)
- `"openai"` - Use OpenAI Moderation API (requires API key)
- `"mock"` - Use keyword-based fallback (for testing)

### Server-Specific Thresholds

Each Discord server has its own toxicity threshold stored in the database:

```python
# Database model (backend/app/database/models.py)
class Server(Base):
    toxicity_threshold = Column(Float, default=0.7)  # 70% confidence
    learning_enabled = Column(Boolean, default=True)
```

**How it works:**
- Message is analyzed â†’ gets a score (0.0 - 1.0)
- If `score >= toxicity_threshold` â†’ Violation triggered
- Threshold can be adjusted per server (0.4 - 0.95)

---

## ðŸ§  Adaptive Learning System

### Automatic Threshold Adjustment

Located in `backend/app/services/adaptive_learning.py`:

```python
class AdaptiveLearningService:
    TARGET_FP_RATE = 0.10  # Target max 10% false positive rate
    ADJUSTMENT_STEP = 0.05  # Adjust by 5% each time
    MIN_THRESHOLD = 0.40    # Never go below 40%
    MAX_THRESHOLD = 0.95    # Never go above 95%
    
    async def process_server_learning(self, server_id: str):
        # Get violations with feedback from last 30 days
        feedback_stats = get_violations_with_feedback(server_id)
        
        # Calculate false positive rate
        fp_rate = false_positives / total_feedback
        
        # Adjust threshold based on FP rate
        if fp_rate > TARGET_FP_RATE:
            # Too many false alarms â†’ Increase threshold (less sensitive)
            new_threshold = min(current + 0.05, MAX_THRESHOLD)
        
        elif fp_rate < (TARGET_FP_RATE / 2):
            # Very few false alarms â†’ Decrease threshold (more sensitive)
            new_threshold = max(current - 0.05, MIN_THRESHOLD)
```

**Example:**
1. Server starts with threshold = 0.70 (70%)
2. Moderators mark 15% of violations as false positives
3. System increases threshold to 0.75 (less sensitive)
4. False positive rate drops to 8%
5. System maintains threshold at 0.75

---

## ðŸš€ Performance Optimizations

### 1. Quick Local Scan (Pre-filtering)

Before running the expensive ML model, a quick keyword scan is performed:

```python
async def quick_local_scan(self, text: str) -> float:
    """Quick local pre-filtering to reduce processing load"""
    
    risk_keywords = [
        'hate', 'kill', 'sex', 'nude', 'fuck', 'shit', 
        'stupid', 'idiot', 'bitch', 'asshole'
    ]
    
    keyword_matches = sum(1 for keyword in risk_keywords if keyword in text.lower())
    risk_score = min(keyword_matches * 0.25, 1.0)
    
    return risk_score
```

**Benefit:** If risk_score is very low, can skip full ML analysis for obvious safe messages.

### 2. Async Thread Execution

ML inference runs in a separate thread pool to avoid blocking:

```python
async def _run_in_thread(self, func, *args, **kwargs):
    """Python 3.8 compatible version of asyncio.to_thread"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(self.executor, func, *args, **kwargs)
```

### 3. Model Caching

- Model is loaded once and kept in memory
- Subsequent analyses reuse the loaded model
- Cache directory: `./models/huggingface/`

---

## ðŸ”„ Fallback Mechanisms

### 3-Tier Fallback System

1. **Primary:** HuggingFace Transformers (local ML)
2. **Secondary:** OpenAI API (if configured)
3. **Tertiary:** Keyword-based analysis (emergency fallback)

```python
def get_content_analyzer():
    analyzer_type = config.CONTENT_ANALYZER.lower()
    
    if analyzer_type == "huggingface":
        try:
            from app.ml.huggingface_analyzer import huggingface_analyzer
            return huggingface_analyzer
        except ImportError:
            # Fall back to mock analyzer
            from app.ml.mock_content_analyzer import mock_content_analyzer
            return mock_content_analyzer
```

---

## ðŸ“¦ Dependencies Required

From `backend/requirements.txt`:

```txt
# ML/AI Libraries
transformers==4.x.x      # Hugging Face Transformers
torch==2.x.x             # PyTorch (backend for transformers)
openai==1.52.0           # Optional: OpenAI API

# Core Dependencies
fastapi==0.104.1         # Web framework
discord.py==2.3.2        # Discord bot
sqlalchemy==2.0.23       # Database ORM
```

**Installation:**
```bash
pip install transformers torch
```

**First Run:**
- Downloads `unitary/toxic-bert` model (~400MB)
- Cached in `~/.cache/huggingface/` or custom directory
- Subsequent runs use cached model

---

## ðŸŽ¯ Violation Categories

The system maps ML outputs to these violation types:

| Category | Triggers | Example |
|----------|----------|---------|
| `toxicity` | harassment, hate speech, insults | "You're an idiot" |
| `nsfw` | sexual content, explicit language | "Check out this porn" |
| `threats` | violence, threatening language | "I'll kill you" |
| `self_harm` | suicide, self-harm mentions | "I want to die" |
| `spam` | repetitive messages, excessive caps | "BUY NOW!!!" (x10) |

---

## ðŸ” Example Flow

### Message: "You're such a fucking idiot lol"

```
1. Quick Scan:
   - Detects: "fucking", "idiot"
   - Risk score: 0.50 â†’ Proceed to full analysis

2. HuggingFace Analysis:
   - toxic_score: 0.85
   - insult_score: 0.89
   - obscene_score: 0.78
   
3. Keyword Enhancement:
   - Detects "lol" (friendly context)
   - Detects "fucking", "idiot" (toxic words)
   - Applies 85% protection (no reduction for toxic content)
   
4. Final Scores:
   - harassment: 0.89
   - max_score: 0.89
   - violation_type: "toxicity"
   
5. Threshold Check:
   - Server threshold: 0.70
   - 0.89 >= 0.70 â†’ VIOLATION TRIGGERED
   
6. Action:
   - Delete message
   - Warn user (1st warning)
   - Log to database
```

---

## ðŸ› ï¸ Customization Options

### 1. Change ML Model

Edit `backend/app/ml/huggingface_analyzer.py`:

```python
# Current model
model="unitary/toxic-bert"

# Alternative models:
# model="martin-ha/toxic-comment-model"
# model="unitary/unbiased-toxic-roberta"
```

### 2. Adjust Keyword Lists

Edit keyword detection methods:

```python
def _check_harassment_keywords(self, text: str) -> bool:
    harassment_keywords = [
        'stupid', 'idiot', 'moron',
        # Add your custom keywords here
    ]
```

### 3. Modify Adjustment Logic

Edit `_apply_keyword_adjustments()` to change how context affects scores.

---

## ðŸ“ˆ Monitoring & Debugging

### Logging

The system provides detailed logs:

```
ðŸ¤— HF analysis: flagged=true, score=0.890, type=toxicity
ðŸ”§ Applied adjustments: toxic_content_protection â†’ factor=0.85
ðŸš¨ VIOLATION TRIGGERED: 0.890 >= 0.700
```

### Model Info in Response

Every analysis includes model metadata:

```json
"model_info": {
  "provider": "Hugging Face",
  "model": "unitary/toxic-bert",
  "toxic_score": 0.85,
  "insult_score": 0.89,
  "original_max_score": 0.92,
  "adjustment_applied": true
}
```

---

## ðŸŽ“ Summary

**Your toxicity detection system:**

âœ… **Uses LOCAL ML models** (Hugging Face Transformers)  
âœ… **Privacy-preserving** (no data sent to external APIs by default)  
âœ… **Free to use** (no API costs)  
âœ… **Adaptive** (learns from feedback)  
âœ… **Multi-layered** (ML + keywords + context)  
âœ… **Server-specific** (customizable thresholds)  
âœ… **Production-ready** (async, fallbacks, caching)

**Model:** `unitary/toxic-bert` (BERT-based classifier)  
**Categories:** 6 toxic categories + keyword enhancements  
**Performance:** ~100-200ms per message (CPU)  
**Accuracy:** High precision with adaptive learning

---

## ðŸ”— Related Files

- `backend/app/ml/huggingface_analyzer.py` - Main ML analyzer
- `backend/app/ml/content_analyzer.py` - Analyzer factory/selector
- `backend/app/services/adaptive_learning.py` - Threshold adjustment
- `backend/app/bot/discord_bot_minimal.py` - Discord integration
- `backend/app/utils/config.py` - Configuration settings

---

**Questions?** Check the inline comments in the code or review the logs for detailed analysis flow!
